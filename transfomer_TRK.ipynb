{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a7401c55",
      "metadata": {
        "id": "a7401c55"
      },
      "source": [
        "# Transformer Model Training\n",
        "This notebook demonstrates training a transformer model for lens performance prediction using PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50df45ac",
      "metadata": {
        "id": "50df45ac"
      },
      "source": [
        "### Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clone the repository\n",
        "!git clone https://github.com/ruthshiferaw/2.156-Lens-Project.git\n",
        "\n",
        "# 2. Navigate into the folder (This is crucial!)\n",
        "%cd 2.156-Lens-Project\n",
        "\n",
        "# 3. Verify you see your files\n",
        "!ls"
      ],
      "metadata": {
        "id": "O20KHKEWXn40",
        "outputId": "e9ec0eae-fd3c-4029-fc13-971ac562f29b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "O20KHKEWXn40",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '2.156-Lens-Project'...\n",
            "remote: Enumerating objects: 16152, done.\u001b[K\n",
            "remote: Counting objects: 100% (781/781), done.\u001b[K\n",
            "remote: Compressing objects: 100% (767/767), done.\u001b[K\n",
            "remote: Total 16152 (delta 38), reused 17 (delta 14), pack-reused 15371 (from 4)\u001b[K\n",
            "Receiving objects: 100% (16152/16152), 281.80 MiB | 17.06 MiB/s, done.\n",
            "Resolving deltas: 100% (4347/4347), done.\n",
            "Updating files: 100% (13224/13224), done.\n",
            "/content/2.156-Lens-Project/2.156-Lens-Project\n",
            " artifacts\t\t\t\t    'Papers Repo'\n",
            " best_surface_transformer_imputed_mean.pth  'Prime Lenses + Data'\n",
            " best_surface_transformer_no_impute.pth      PythonZOSConnection.py\n",
            "'Data Extraction Routines'\t\t     RemovingFailed.py\n",
            " lens.code-workspace\t\t\t     requirements.txt\n",
            "'Lens Project.pdf'\t\t\t     transfomer_TRK_1056PM1205.ipynb\n",
            " lens_transformer_model.pth\t\t     transfomer_TRK.ipynb\n",
            "'Mid-Project Progress Report.docx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "99ebf4b3",
      "metadata": {
        "id": "99ebf4b3",
        "outputId": "be1d53df-a10a-4ce9-ddd3-974e11e10907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda\n",
            "Loading summary from: /content/2.156-Lens-Project/2.156-Lens-Project/Prime Lenses + Data/CSVExports/file_lens_summary.csv\n",
            "Loading lens data from: /content/2.156-Lens-Project/2.156-Lens-Project/Prime Lenses + Data/LensDataExports\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. SETUP & PATHS ---\n",
        "# Detect GPU (Mac uses 'mps', NVIDIA uses 'cuda', else 'cpu')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Training on: {device}\")\n",
        "\n",
        "current_folder = os.getcwd()\n",
        "data_folder = os.path.join(current_folder, \"Prime Lenses + Data\", \"LensDataExports\")\n",
        "summary_file = os.path.join(current_folder, \"Prime Lenses + Data\", \"CSVExports\", \"file_lens_summary.csv\")\n",
        "\n",
        "print(f\"Loading summary from: {summary_file}\")\n",
        "print(f\"Loading lens data from: {data_folder}\")\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2480e766",
      "metadata": {
        "id": "2480e766"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. DATASET (FIXED: MASK LOGIC)\n",
        "# ==========================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "\n",
        "def index_lens_files(root_dir, ignore_files=None):\n",
        "    if ignore_files is None: ignore_files = []\n",
        "    path_map = {}\n",
        "    print(f\"Scanning {root_dir} for lens files...\")\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".csv\"):\n",
        "                if file in ignore_files: continue\n",
        "                lens_name = os.path.splitext(file)[0]\n",
        "                full_path = os.path.join(root, file)\n",
        "                path_map[lens_name] = full_path\n",
        "    print(f\"Found {len(path_map)} lens files.\")\n",
        "    return path_map\n",
        "\n",
        "class LensPerformanceDataset(Dataset):\n",
        "    def __init__(self, summary_file, lens_data_root, material_vocab=None):\n",
        "        self.summary_df = pd.read_csv(summary_file)\n",
        "        summary_filename = os.path.basename(summary_file)\n",
        "        self.file_path_map = index_lens_files(lens_data_root, ignore_files=[summary_filename])\n",
        "\n",
        "        self.surface_columns = ['Surface', 'TypeName', 'Comment', 'Radius', 'Thickness', 'Material', 'SemiDiameter']\n",
        "        self.numeric_features = ['Radius', 'Thickness', 'SemiDiameter']\n",
        "        self.categorical_feature = 'Material'\n",
        "\n",
        "        # Dynamic Targets\n",
        "        all_numeric = self.summary_df.select_dtypes(include=['number']).columns.tolist()\n",
        "        ignore_cols = ['File Name', 'Unnamed: 0']\n",
        "        self.target_cols = [c for c in all_numeric if c not in ignore_cols]\n",
        "        print(f\"ðŸŽ¯ Auto-Detected {len(self.target_cols)} Targets\")\n",
        "\n",
        "        if material_vocab is None:\n",
        "            self.material_vocab = self._build_vocab()\n",
        "        else:\n",
        "            self.material_vocab = material_vocab\n",
        "\n",
        "        self.input_stats = self._compute_input_stats()\n",
        "        self.target_stats = self._compute_target_stats()\n",
        "\n",
        "    def _build_vocab(self):\n",
        "        unique_materials = set(['Air'])\n",
        "        for lens_name, file_path in self.file_path_map.items():\n",
        "            try:\n",
        "                df = pd.read_csv(file_path, usecols=[self.categorical_feature])\n",
        "                unique_materials.update(df[self.categorical_feature].astype(str).unique())\n",
        "            except: pass\n",
        "        return {name: i for i, name in enumerate(sorted(unique_materials))}\n",
        "\n",
        "    def _safe_preprocess(self, df):\n",
        "        data = df[self.numeric_features].values.astype(np.float32)\n",
        "\n",
        "        # Radius -> Curvature\n",
        "        radius = data[:, 0]\n",
        "        radius = np.where(np.abs(radius) > 1e6, np.inf, radius)\n",
        "        with np.errstate(divide='ignore'):\n",
        "            curvature = np.where(np.isfinite(radius) & (radius != 0), 1.0 / radius, 0.0)\n",
        "        data[:, 0] = curvature\n",
        "\n",
        "        # Clamp & Log1p\n",
        "        data[:, 1] = np.log1p(np.clip(data[:, 1], 0, 1000.0))\n",
        "        data[:, 2] = np.log1p(np.clip(data[:, 2], 0, 1000.0))\n",
        "        return data\n",
        "\n",
        "    def _compute_input_stats(self):\n",
        "        print(\"Computing INPUT stats...\")\n",
        "        all_data = []\n",
        "        sample_keys = list(self.file_path_map.keys())[:300]\n",
        "        for key in sample_keys:\n",
        "            try:\n",
        "                path = self.file_path_map[key]\n",
        "                df = pd.read_csv(path, usecols=self.numeric_features)\n",
        "                df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "                processed = self._safe_preprocess(df)\n",
        "                all_data.append(processed)\n",
        "            except: pass\n",
        "\n",
        "        if not all_data: return {'mean': np.zeros(3), 'std': np.ones(3)}\n",
        "        all_data = np.vstack(all_data)\n",
        "        mean = np.mean(all_data, axis=0)\n",
        "        std = np.std(all_data, axis=0)\n",
        "        std = np.where(std < 1e-6, 1.0, std)\n",
        "        print(f\"Inputs -> Mean: {mean}, Std: {std}\")\n",
        "        return {'mean': mean, 'std': std}\n",
        "\n",
        "    def _compute_target_stats(self):\n",
        "        print(\"Computing TARGET stats...\")\n",
        "        target_vals = self.summary_df[self.target_cols].values.astype(np.float32)\n",
        "        target_vals = np.nan_to_num(target_vals, nan=0.0)\n",
        "        mean = np.mean(target_vals, axis=0)\n",
        "        std = np.std(target_vals, axis=0)\n",
        "        std = np.where(std < 1e-6, 1.0, std)\n",
        "        return {'mean': torch.tensor(mean, dtype=torch.float32),\n",
        "                'std': torch.tensor(std, dtype=torch.float32)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.summary_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.summary_df.iloc[idx]\n",
        "        lens_name = row['File Name']\n",
        "        lookup_name = lens_name + \"_LensData\"\n",
        "\n",
        "        if lookup_name not in self.file_path_map:\n",
        "            if lens_name in self.file_path_map: lookup_name = lens_name\n",
        "            else: return None\n",
        "\n",
        "        try:\n",
        "            lens_df = pd.read_csv(self.file_path_map[lookup_name], usecols=self.surface_columns)\n",
        "            lens_df[self.numeric_features] = lens_df[self.numeric_features].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "            numeric_data = self._safe_preprocess(lens_df)\n",
        "            norm_numeric = (numeric_data - self.input_stats['mean']) / (self.input_stats['std'])\n",
        "            lens_numeric = torch.tensor(norm_numeric, dtype=torch.float32)\n",
        "\n",
        "            materials = lens_df[self.categorical_feature].astype(str).map(self.material_vocab).fillna(0)\n",
        "            lens_material_ids = torch.tensor(materials.values, dtype=torch.long)\n",
        "\n",
        "            raw_targets = row[self.target_cols].values.astype(float)\n",
        "            raw_targets = np.nan_to_num(raw_targets, nan=0.0)\n",
        "            raw_t_tensor = torch.tensor(raw_targets, dtype=torch.float32)\n",
        "            norm_targets = (raw_t_tensor - self.target_stats['mean']) / self.target_stats['std']\n",
        "\n",
        "            return {'numeric_seq': lens_numeric, 'material_seq': lens_material_ids, 'targets': norm_targets}\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "# --- FIXED COLLATE FUNCTION ---\n",
        "def lens_collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if len(batch) == 0: return None\n",
        "\n",
        "    numeric_seqs = [item['numeric_seq'] for item in batch]\n",
        "    material_seqs = [item['material_seq'] for item in batch]\n",
        "    targets = [item['targets'] for item in batch]\n",
        "\n",
        "    padded_numeric = pad_sequence(numeric_seqs, batch_first=True, padding_value=0.0)\n",
        "    padded_materials = pad_sequence(material_seqs, batch_first=True, padding_value=0)\n",
        "\n",
        "    # === CRITICAL FIX: True = PADDING (Ignore), False = DATA (Keep) ===\n",
        "    # We check if numeric is all zeros (padding) AND material is 0 (padding)\n",
        "    padding_mask = (padded_materials == 0) & (padded_numeric.abs().sum(dim=2) == 0)\n",
        "\n",
        "    return {'numeric_seq': padded_numeric, 'material_seq': padded_materials, 'mask': padding_mask, 'targets': torch.stack(targets)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "091366cb",
      "metadata": {
        "id": "091366cb"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. TRANSFORMER MODEL\n",
        "# ==========================================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class LensTransformer(nn.Module):\n",
        "    def __init__(self, num_materials, d_model=64, nhead=4, num_layers=3, output_dim=2):\n",
        "        super().__init__()\n",
        "        # 1. Embeddings\n",
        "        self.mat_embed = nn.Embedding(num_materials, 16)\n",
        "        self.num_embed = nn.Linear(3, d_model - 16)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # 2. Transformer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # 3. Prediction Head (Using the Mean-Pooled Giga Vector)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, numeric_seq, material_seq, src_key_padding_mask):\n",
        "        # Embed\n",
        "        mat_vecs = self.mat_embed(material_seq)\n",
        "        num_vecs = self.num_embed(numeric_seq)\n",
        "        x = torch.cat([num_vecs, mat_vecs], dim=2)\n",
        "\n",
        "        # Transform\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Mean Pooling (Collapse N surfaces -> 1 Giga Vector)\n",
        "        valid_mask = (~src_key_padding_mask).unsqueeze(-1).float()\n",
        "        sum_embeddings = (x * valid_mask).sum(dim=1)\n",
        "        num_valid = valid_mask.sum(dim=1)\n",
        "        lens_vector = sum_embeddings / (num_valid + 1e-9)\n",
        "\n",
        "        # Predict\n",
        "        return self.head(lens_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d5ecb6d9",
      "metadata": {
        "id": "d5ecb6d9",
        "outputId": "8aef897a-c7bd-44a0-8cbc-1b1b8e044000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning /content/2.156-Lens-Project/2.156-Lens-Project/Prime Lenses + Data/LensDataExports for lens files...\n",
            "Found 1019 lens files.\n",
            "ðŸŽ¯ Auto-Detected 11 Targets\n",
            "Computing INPUT stats...\n",
            "Inputs -> Mean: [0.01351134 1.3206878  2.4531727 ], Std: [0.28659356 1.0645258  0.94376355]\n",
            "Computing TARGET stats...\n",
            "Vocab Size: 1554\n",
            "Predicting 11 metrics\n",
            "\n",
            "--- Starting Training ---\n",
            "Epoch [1/50] Train Loss: 0.0000 | Test Loss: 0.0515\n",
            "Epoch [5/50] Train Loss: 0.0000 | Test Loss: 0.0414\n",
            "Epoch [10/50] Train Loss: 0.0216 | Test Loss: 0.0364\n",
            "Epoch [15/50] Train Loss: 0.0000 | Test Loss: 0.0364\n",
            "Epoch [20/50] Train Loss: 0.0000 | Test Loss: 0.0316\n",
            "Epoch [25/50] Train Loss: 0.0000 | Test Loss: 0.0284\n",
            "Epoch [30/50] Train Loss: 0.0021 | Test Loss: 0.0279\n",
            "Epoch [35/50] Train Loss: 0.0000 | Test Loss: 0.0279\n",
            "Epoch [40/50] Train Loss: 0.0866 | Test Loss: 0.0287\n",
            "Epoch [45/50] Train Loss: 0.0000 | Test Loss: 0.0323\n",
            "Epoch [50/50] Train Loss: 0.0000 | Test Loss: 0.0346\n",
            "Done! Saving model...\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 4. TRAINING LOOP\n",
        "# ==========================================\n",
        "\n",
        "full_dataset = LensPerformanceDataset(summary_file, data_folder)\n",
        "if len(full_dataset) == 0: raise ValueError(\"Dataset is empty! Check paths.\")\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lens_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=lens_collate_fn)\n",
        "\n",
        "# Dynamic Config\n",
        "num_materials = len(full_dataset.material_vocab)\n",
        "num_targets = len(full_dataset.target_cols)\n",
        "\n",
        "print(f\"Vocab Size: {num_materials}\")\n",
        "print(f\"Predicting {num_targets} metrics\")\n",
        "\n",
        "# Model\n",
        "model = LensTransformer(num_materials=num_materials, d_model=64, output_dim=num_targets).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        if batch is None: continue\n",
        "\n",
        "        # Safety Check\n",
        "        if (torch.isnan(batch['numeric_seq']).any() or torch.isinf(batch['numeric_seq']).any() or batch['mask'].sum() == 0):\n",
        "            continue\n",
        "\n",
        "        numeric = batch['numeric_seq'].to(device)\n",
        "        material = batch['material_seq'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(numeric, material, mask)\n",
        "        loss = criterion(predictions, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # --- KEY FIX: GRADIENT CLIPPING ---\n",
        "        # If gradients explode, clip them to 1.0\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        if (torch.isnan(predictions).any() or torch.isinf(predictions).any()):\n",
        "            print(\"NaN detected! Clipping saved us.\")\n",
        "            continue\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
        "\n",
        "    model.eval()\n",
        "    running_test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            if batch is None: continue\n",
        "            numeric = batch['numeric_seq'].to(device)\n",
        "            material = batch['material_seq'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            loss = criterion(model(numeric, material, mask), targets)\n",
        "            running_test_loss += loss.item()\n",
        "\n",
        "    avg_test_loss = running_test_loss / len(test_loader) if len(test_loader) > 0 else 0.0\n",
        "\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch+1}/50] Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "print(\"Done! Saving model...\")\n",
        "torch.save(model.state_dict(), \"lens_transformer_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9zeENz-TY51J"
      },
      "id": "9zeENz-TY51J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "594fb4f3",
      "metadata": {
        "id": "594fb4f3",
        "outputId": "639c7791-5674-4757-a356-130f6addd45d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: None\n",
            "Batch 1: None\n",
            "Batch 2: None\n",
            "Batch 3: None\n",
            "Batch 4: None\n",
            "Batch 5: Skipped due to NaN/Inf or empty mask\n",
            "Batch 6: None\n",
            "Batch 7: Skipped due to NaN/Inf or empty mask\n",
            "Batch 8: None\n",
            "Batch 9: None\n",
            "Batch 10: None\n",
            "Batch 11: None\n",
            "Batch 12: None\n",
            "Batch 13: None\n",
            "Batch 14: None\n",
            "Batch 15: None\n",
            "Batch 16: None\n",
            "Batch 17: None\n",
            "Batch 18: None\n",
            "Batch 19: None\n",
            "Total non-skipped batches found: 0\n"
          ]
        }
      ],
      "source": [
        "# old code predicting just 2\n",
        "# ==========================================\n",
        "# DIAGNOSTIC: Count and inspect non-skipped batches\n",
        "# ==========================================\n",
        "processed_batches = 0\n",
        "loader_iter = iter(train_loader)\n",
        "for i in range(20):  # Check up to 20 batches\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "        if batch is None:\n",
        "            print(f\"Batch {i}: None\")\n",
        "            continue\n",
        "        if (\n",
        "            torch.isnan(batch['numeric_seq']).any() or\n",
        "            torch.isinf(batch['numeric_seq']).any() or\n",
        "            torch.isnan(batch['material_seq']).any() or\n",
        "            torch.isinf(batch['material_seq']).any() or\n",
        "            torch.isnan(batch['targets']).any() or\n",
        "            torch.isinf(batch['targets']).any() or\n",
        "            batch['mask'].sum() == 0\n",
        "        ):\n",
        "            print(f\"Batch {i}: Skipped due to NaN/Inf or empty mask\")\n",
        "            continue\n",
        "        processed_batches += 1\n",
        "        print(f\"Batch {i}: PROCESSED\")\n",
        "        print(\"numeric_seq:\", batch['numeric_seq'])\n",
        "        print(\"material_seq:\", batch['material_seq'])\n",
        "        print(\"mask:\", batch['mask'])\n",
        "        print(\"targets:\", batch['targets'])\n",
        "        break  # Only print the first valid batch\n",
        "    except StopIteration:\n",
        "        print(f\"Batch {i}: End of loader\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Batch {i}: Exception {e}\")\n",
        "print(f\"Total non-skipped batches found: {processed_batches}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ac7115a0",
      "metadata": {
        "id": "ac7115a0",
        "outputId": "64b2a29d-326c-44cf-e23d-6f9a4c160dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'path_map' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3990929117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(path_map)} lens files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'path_map' is not defined"
          ]
        }
      ],
      "source": [
        "print(f\"Found {len(path_map)} lens files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42bbec4",
      "metadata": {
        "id": "a42bbec4"
      },
      "source": [
        "## Debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fd7552",
      "metadata": {
        "id": "f1fd7552"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BATCH AND TARGET DIAGNOSTICS BEFORE TRAINING\n",
        "# ==========================================\n",
        "loader_iter = iter(train_loader)\n",
        "for i in range(3):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "        if batch is None:\n",
        "            print(f\"Batch {i}: None\")\n",
        "            continue\n",
        "         print(f\"Batch {i}: Numeric shape {batch['numeric_seq'].shape}, Material shape {batch['material_seq'].shape}, Targets shape {batch['targets'].shape}\")\n",
        "        print(f\"Batch {i} targets: {batch['targets']}\")\n",
        "        # Check for NaNs/Infs\n",
        "        for key in ['numeric_seq', 'material_seq', 'targets']:\n",
        "            arr = batch[key]\n",
        "            if torch.isnan(arr).any():\n",
        "                print(f\"Batch {i}: {key} contains NaNs!\")\n",
        "            if torch.isinf(arr).any():\n",
        "                print(f\"Batch {i}: {key} contains Infs!\")\n",
        "    except StopIteration:\n",
        "        print(f\"Batch {i}: End of loader\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Batch {i}: Exception {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af11b29",
      "metadata": {
        "id": "4af11b29"
      },
      "source": [
        "Model training complete. The transformer weights are saved to `lens_transformer_model.pth`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45b8cc20",
      "metadata": {
        "id": "45b8cc20"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# DEBUGGING DATASET AND DATALOADER\n",
        "# ==========================================\n",
        "print(\"\\n--- Dataset Debugging ---\")\n",
        "invalid_count = 0\n",
        "valid_count = 0\n",
        "for idx in range(len(full_dataset)):\n",
        "    row = full_dataset.summary_df.iloc[idx]\n",
        "    lens_name = row['File Name']+\"_LensData\"\n",
        "    print(f\"Index {idx}: Lens Name = {lens_name}\", end=' ')\n",
        "    if lens_name not in full_dataset.file_path_map:\n",
        "        print(\"[MISSING FILE]\")\n",
        "        invalid_count += 1\n",
        "        continue\n",
        "    try:\n",
        "        item = full_dataset[idx]\n",
        "        if item is None:\n",
        "            print(\"[ITEM NONE]\")\n",
        "            invalid_count += 1\n",
        "        else:\n",
        "            print(\"[VALID]\")\n",
        "            valid_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"[EXCEPTION] {e}\")\n",
        "        invalid_count += 1\n",
        "print(f\"\\nTotal valid items: {valid_count}\")\n",
        "print(f\"Total invalid/missing items: {invalid_count}\")\n",
        "\n",
        "print(\"\\n--- DataLoader Batch Debugging ---\")\n",
        "loader_iter = iter(train_loader)\n",
        "for i in range(3):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "        if batch is None:\n",
        "            print(f\"Batch {i}: None\")\n",
        "        else:\n",
        "            print(f\"Batch {i}: Numeric shape {batch['numeric_seq'].shape}, Material shape {batch['material_seq'].shape}, Targets shape {batch['targets'].shape}\")\n",
        "    except StopIteration:\n",
        "        print(f\"Batch {i}: End of loader\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Batch {i}: Exception {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae633138",
      "metadata": {
        "id": "ae633138"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# BATCH AND TARGET DIAGNOSTICS BEFORE TRAINING\n",
        "# ==========================================\n",
        "loader_iter = iter(train_loader)\n",
        "for i in range(3):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "        if batch is None:\n",
        "            print(f\"Batch {i}: None\")\n",
        "            continue\n",
        "        print(f\"Batch {i}: Numeric shape {batch['numeric_seq'].shape}, Material shape {batch['material_seq'].shape}, Targets shape {batch['targets'].shape}\")\n",
        "        print(f\"Batch {i} targets: {batch['targets']}\")\n",
        "        # Check for NaNs/Infs\n",
        "        for key in ['numeric_seq', 'material_seq', 'targets']:\n",
        "            arr = batch[key]\n",
        "            if torch.isnan(arr).any():\n",
        "                print(f\"Batch {i}: {key} contains NaNs!\")\n",
        "            if torch.isinf(arr).any():\n",
        "                print(f\"Batch {i}: {key} contains Infs!\")\n",
        "    except StopIteration:\n",
        "        print(f\"Batch {i}: End of loader\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Batch {i}: Exception {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}